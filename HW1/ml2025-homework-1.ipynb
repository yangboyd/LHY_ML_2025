{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML2025 Homework 1 - Retrieval Augmented Generation with Agents","metadata":{"id":"1TFwaJir_Olj"}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{"id":"6tQHdH2k_Olk"}},{"cell_type":"markdown","source":"In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB.","metadata":{"id":"mGx000oZ_Oll"}},{"cell_type":"code","source":"!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt","metadata":{"id":"5JywoPOO_Oll","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:22:37.639798Z","iopub.execute_input":"2025-05-11T09:22:37.640079Z","iopub.status.idle":"2025-05-11T09:23:45.690163Z","shell.execute_reply.started":"2025-05-11T09:22:37.640046Z","shell.execute_reply":"2025-05-11T09:23:45.689331Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\nCollecting llama-cpp-python==0.3.4\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl (445.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m199.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\nCollecting googlesearch-python\n  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.4.1)\nCollecting requests-html\n  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\nCollecting lxml_html_clean\n  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\nRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\nCollecting pyquery (from requests-html)\n  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\nCollecting fake-useragent (from requests-html)\n  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\nCollecting parse (from requests-html)\n  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting w3lib (from requests-html)\n  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting pyppeteer>=0.0.14 (from requests-html)\n  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\nCollecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\nRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.5.0)\nCollecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\nCollecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\nCollecting cssselect>=1.2.0 (from pyquery->requests-html)\n  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\nDownloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading requests_html-0.10.0-py3-none-any.whl (13 kB)\nDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\nDownloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\nDownloading pyquery-2.0.1-py3-none-any.whl (22 kB)\nDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\nDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\nDownloading pyee-11.1.1-py3-none-any.whl (15 kB)\nDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n  Attempting uninstall: websockets\n    Found existing installation: websockets 14.1\n    Uninstalling websockets-14.1:\n      Successfully uninstalled websockets-14.1\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.3.0\n    Uninstalling urllib3-2.3.0:\n      Successfully uninstalled urllib3-2.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.2.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.2 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n--2025-05-11 09:23:10--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nResolving huggingface.co (huggingface.co)... 3.163.189.90, 3.163.189.114, 3.163.189.74, ...\nConnecting to huggingface.co (huggingface.co)|3.163.189.90|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250511%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250511T092310Z&X-Amz-Expires=3600&X-Amz-Signature=52b2006a0c68dad36c5ade8d8dbd4d526f4394301ec3da9843b68cd6e39f61bc&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1746958990&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njk1ODk5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=WaiSlfVRdYRW1JN63fTaDGIRutvrFcdYYLjes6D3inI5bh%7EmCHShyl41-9GJVi9z3RzDvWBiujeLslE2P5MYNDxhFyPGd0kTYakJ5Pjow4MgZ%7EoQgEZFUzPTmfCvRR8OIl%7EDvugTj5aLZVAQhny%7EF98ByOJdjOQv%7EyF%7EFFis3UqDqlsfTMuV66ZFyZu69KXrAvU-2Bt96Vp7X0BqXiJcaMAG3rjj-QXXbMwi7Ndc6pcqoXXdXORU2PDDhCYmbjo3EfBE9-3Ze6-7Ut0wRsfDaOWLgeDxtHZVw8ipW5OLpraLWgqEULyYNwoSOJlOKvlpMgCHIvP%7ECYkGAWFS74DDVw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-05-11 09:23:10--  https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250511%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250511T092310Z&X-Amz-Expires=3600&X-Amz-Signature=52b2006a0c68dad36c5ade8d8dbd4d526f4394301ec3da9843b68cd6e39f61bc&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1746958990&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Njk1ODk5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=WaiSlfVRdYRW1JN63fTaDGIRutvrFcdYYLjes6D3inI5bh%7EmCHShyl41-9GJVi9z3RzDvWBiujeLslE2P5MYNDxhFyPGd0kTYakJ5Pjow4MgZ%7EoQgEZFUzPTmfCvRR8OIl%7EDvugTj5aLZVAQhny%7EF98ByOJdjOQv%7EyF%7EFFis3UqDqlsfTMuV66ZFyZu69KXrAvU-2Bt96Vp7X0BqXiJcaMAG3rjj-QXXbMwi7Ndc6pcqoXXdXORU2PDDhCYmbjo3EfBE9-3Ze6-7Ut0wRsfDaOWLgeDxtHZVw8ipW5OLpraLWgqEULyYNwoSOJlOKvlpMgCHIvP%7ECYkGAWFS74DDVw__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.64, 18.238.217.126, 18.238.217.88, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.64|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8540775840 (8.0G)\nSaving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n\nMeta-Llama-3.1-8B-I 100%[===================>]   7.95G   324MB/s    in 23s     \n\n2025-05-11 09:23:34 (352 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n\n--2025-05-11 09:23:34--  https://www.csie.ntu.edu.tw/~ulin/public.txt\nResolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\nConnecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4399 (4.3K) [text/plain]\nSaving to: ‘public.txt’\n\npublic.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n\n2025-05-11 09:23:34 (177 MB/s) - ‘public.txt’ saved [4399/4399]\n\n--2025-05-11 09:23:44--  https://www.csie.ntu.edu.tw/~ulin/private.txt\nResolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\nConnecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 15229 (15K) [text/plain]\nSaving to: ‘private.txt’\n\nprivate.txt         100%[===================>]  14.87K  --.-KB/s    in 0.1s    \n\n2025-05-11 09:23:45 (121 KB/s) - ‘private.txt’ saved [15229/15229]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')","metadata":{"id":"kX6SizAt_Olm","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:23:45.691140Z","iopub.execute_input":"2025-05-11T09:23:45.691404Z","iopub.status.idle":"2025-05-11T09:23:50.109325Z","shell.execute_reply.started":"2025-05-11T09:23:45.691381Z","shell.execute_reply":"2025-05-11T09:23:50.108422Z"}},"outputs":[{"name":"stdout","text":"You are good to go!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Prepare the LLM and LLM utility function","metadata":{"id":"l3iyc1qC_Olm"}},{"cell_type":"markdown","source":"By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models.","metadata":{"id":"T59vxAo2_Olm"}},{"cell_type":"markdown","source":"In the following code block, we will load the downloaded LLM model weights onto the GPU first.\nThen, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily.","metadata":{"id":"vtepTeT3_Olm"}},{"cell_type":"markdown","source":"You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning.","metadata":{"id":"eVil2Vhe_Olm"}},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    This function will inference the model with given messages.\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,    # This argument is how many tokens the model can generate.\n        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"id":"ScyW45N__Olm","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:23:50.110280Z","iopub.execute_input":"2025-05-11T09:23:50.110655Z","iopub.status.idle":"2025-05-11T09:23:54.164565Z","shell.execute_reply.started":"2025-05-11T09:23:50.110633Z","shell.execute_reply":"2025-05-11T09:23:54.163615Z"}},"outputs":[{"name":"stderr","text":"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Search Tool","metadata":{"id":"tnHLwq-4_Olm"}},{"cell_type":"markdown","source":"The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections.","metadata":{"id":"SYM-2ZsE_Olm"}},{"cell_type":"code","source":"from typing import List\nfrom googlesearch import search as _search\nfrom bs4 import BeautifulSoup\nfrom charset_normalizer import detect\nimport asyncio\nfrom requests_html import AsyncHTMLSession\nimport urllib3\nurllib3.disable_warnings()\n\nasync def worker(s:AsyncHTMLSession, url:str):\n    try:\n        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n            return None\n        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n        return r.text\n    except:\n        return None\n\nasync def get_htmls(urls):\n    session = AsyncHTMLSession()\n    tasks = (worker(session, url) for url in urls)\n    return await asyncio.gather(*tasks)\n\nasync def search(keyword: str, n_results: int=3) -> List[str]:\n    '''\n    This function will search the keyword and return the text content in the first n_results web pages.\n    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n    '''\n    keyword = keyword[:100]\n    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n    results = await get_htmls(results)\n    # Filter out the None values.\n    results = [x for x in results if x is not None]\n    # Parse the HTML.\n    results = [BeautifulSoup(x, 'html.parser') for x in results]\n    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n    # Return the first n results.\n    return results[:n_results]","metadata":{"id":"bEIRmZl7_Oln","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:23:54.166663Z","iopub.execute_input":"2025-05-11T09:23:54.167104Z","iopub.status.idle":"2025-05-11T09:23:54.610566Z","shell.execute_reply.started":"2025-05-11T09:23:54.167080Z","shell.execute_reply":"2025-05-11T09:23:54.609872Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Test the LLM inference pipeline","metadata":{"id":"rC3zQjjj_Oln"}},{"cell_type":"code","source":"# You can try out different questions here.\ntest_question='請問誰是 Taylor Swift？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": test_question}, # User prompt\n]\n\nprint(generate_response(llama3, messages))","metadata":{"id":"8dmGCARd_Oln","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:23:54.611742Z","iopub.execute_input":"2025-05-11T09:23:54.612203Z","iopub.status.idle":"2025-05-11T09:24:02.675974Z","shell.execute_reply.started":"2025-05-11T09:23:54.612166Z","shell.execute_reply":"2025-05-11T09:24:02.674960Z"}},"outputs":[{"name":"stdout","text":"泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村乐逐渐转变为流行摇滚，并且她被誉為當代最成功的女艺人的之一。\n\n泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，並於2006年發布首張專輯《Taylor Swift》。隨後，她推出了多张专辑，包括 《Fearless》（勇敢）、_Speak Now（說出來）和 _1989 等。她以她的歌曲如 \"Shake It Off\"、\"_Blank Space_\" 和 \"_Bad Blood 》等获得了广泛的认可。\n\n泰勒絲也是一位頗具爭議性的藝人，她曾經與多個音樂家發生過創作權和商業糾紛。然而，無論如何她都成為了一代人的偶像，並且她的音乐影響力在全球各地都是非常大的！\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Agents","metadata":{"id":"C0-ojJuE_Oln"}},{"cell_type":"markdown","source":"The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n- Attributes:\n    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n    - llm: Just an indicator of the LLM model used by the agent.\n- Method:\n    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output.","metadata":{"id":"HGsIPud3_Oln"}},{"cell_type":"code","source":"class LLMAgent():\n    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n        self.task_description = task_description    # Task description instructs what task should this agent solve.\n        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n    def inference(self, message:str) -> str:\n        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n            # TODO: Design the system prompt and user prompt here.\n            # Format the messsages first.\n            messages = [\n                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n            ]\n            return generate_response(llama3, messages)\n        else:\n            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n            return \"\"","metadata":{"id":"zjG-UwDX_Oln","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:24:02.676847Z","iopub.execute_input":"2025-05-11T09:24:02.677212Z","iopub.status.idle":"2025-05-11T09:24:02.682962Z","shell.execute_reply.started":"2025-05-11T09:24:02.677187Z","shell.execute_reply":"2025-05-11T09:24:02.682167Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"TODO 1: Design the role description and task description for each agent.","metadata":{"id":"0-ueJrgP_Oln"}},{"cell_type":"code","source":"# TODO: Design the role and task description for each agent.\n\nSTUDENT_ID = \"200505081050\"\n\n# This agent may help you filter out the irrelevant parts in question descriptions.\nquestion_extraction_agent = LLMAgent(\n    role_description=\"你是一個專業的問題提煉專家，能夠過濾問題中的背景描述，但必須完整保留所有專有名詞、地名和問題的具體查詢目標\",\n    task_description=\"請提煉以下問題的核心，過濾掉多餘的背景描述，但必須嚴格遵守：1. 保留所有專有名詞和地名的完整形式；2. 絕對不可更改問題的查詢目標（如「哪間科技公司」不可簡化為「誰發明了」）；3. 保留問題詢問的所有具體要素（如詢問「產品」時不可改為「發明」）；4. 確保提煉後的問題完整表達原問題的精確查詢意圖；5. 直接輸出提煉後的問題，不要包含「提煉後的問題是：」等贅字。例如「Windows 作業系統是哪間科技公司的產品？」不可簡化為「誰發明了 Windows 作業系統？」\",\n)\n\n# This agent analyzes the question and decides the handling method\nplan_agent = LLMAgent(\n    role_description=\"你是一個專業的問題分析專家，能夠判斷問題的類型並決定最佳處理方式，不要直接回答问题\",\n    task_description=\"請分析以下問題並決定最佳的處理方式，嚴格按照以下規則：1. 如果是純數學計算問題，回傳「math」；2. 如果是基礎科學知識、歷史重大事件或地理常識等你絕對確定答案的問題，回傳「direct_response」；3. 如果是流行文化、名人言論、電影台詞、特定產品、時事新聞、體育賽事或任何你不能100%確定答案的問題，必須回傳「web_search」；4. 任何有爭議、需要最新資訊或專業領域知識的問題，回傳「web_search」。只回傳單一關鍵字，不要有任何解釋。\",\n)\n\n# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\nkeyword_extraction_agent = LLMAgent(\n    role_description=\"你是一個專業的搜尋關鍵字提取器，精確提取問題中的所有關鍵資訊，形成有效的web搜索詞組\",\n    task_description=\"請從問題中提取用於web搜索的關鍵字，嚴格遵守以下規則：1. 提取所有重要名詞、事件、人物、時間詞和關鍵概念；2. 必須包含問題的主要事件和情境；3. 必須保留問題的查詢意圖和目標；4. 絕對不可添加問題中不存在的詞語；5. 絕對不可提供問題的預設答案作為關鍵詞；6. 將相關概念組合為有意義的詞組；7. 完全去除虛詞和不影響搜索的介詞；8. 確保關鍵詞能直接用於web搜索且能獲得相關結果；9. 關鍵詞之間用空格分隔；10. 對「是誰」「哪個」等疑問句，必須保留查詢目標而非預測答案。例子：「藝人大S是在去哪個國家旅遊時因病去世？」應提取「大S 國家 旅遊 去世」；「是誰發現了萬有引力？」應提取「萬有引力 發現者」；「最新的輝達顯卡是出到「GeForce RTX 多少」系列？」應提取「輝達 GeForce RTX 最新系列 型號」。請只返回空格分隔的關鍵詞列表\",\n)\n\n# This agent is the core component that answers the question.\nresult_judgment_agent = LLMAgent(\n    role_description=\"你是一個專業的文件相關性評估專家，善於判斷文件對問題的價值\",\n    task_description=\"請判斷以下文件是否包含回答問題所需的關鍵資訊。評估標準：1. 文件必須直接相關於問題的核心要素（如專有名詞、地名）；2. 如果文件有幫助，請過濾掉不相關的內容，僅保留對回答問題有用的關鍵資訊；3. 如果內容確實相關，不必特別說明「True」，直接返回精簡後的相關內容；4. 如果判斷完全沒有幫助，只要回傳「False」。請使用繁體中文回覆。\",\n)\n\n# This agent is the core component that answers the question.\nqa_agent = LLMAgent(\n    role_description=\"你是一個專業的問答專家，擅長根據提供的資料準確回答問題\",\n    task_description=\"請根據提供的文件內容，準確回答問題。回答要求：1. 直接給出明確、具體的答案；2. 如有多個可能答案，請選擇最符合問題要求的；3. 優先使用文件中提供的專業術語和專有名詞；4. 不必說明是根據文件內容回答，直接給出答案即可；5. 回答必須使用繁體中文，語言要精準簡潔。\",\n)\n\n# This agent handles direct responses without requiring web search\ndirect_response_agent = LLMAgent(\n    role_description=\"你是一個知識豐富的問答專家，善於根據已知常識回答問題\",\n    task_description=\"請直接回答以下問題，無需網路搜尋。回答要求：1. 只回答你絕對確定無誤的事實；2. 僅限於基礎科學、歷史重大事件或地理常識等領域的問題；3. 對於流行文化、電影台詞、名人言論等內容，應該明確表示需要查證；4. 如果有任何不確定，請回覆「這個問題需要查證」而非給出可能錯誤的答案；5. 答案必須簡潔精確；6. 回答必須使用繁體中文；7. 如果問題涉及數學計算，請給出計算過程和最終結果。\",\n)\n\n# This agent handles mathematical questions\nmath_agent = LLMAgent(\n    role_description=\"你是一個數學專家，擅長解決各種數學問題\",\n    task_description=\"請解答以下數學問題。回答要求：1. 清晰列出解題思路和計算步驟；2. 給出最終精確答案；3. 對於複雜計算，確保結果的準確性；4. 回答必須使用繁體中文；5. 如適用，可使用數學符號和公式表示。\",\n)","metadata":{"id":"DzPzmNnj_Oln","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:24:02.683857Z","iopub.execute_input":"2025-05-11T09:24:02.684196Z","iopub.status.idle":"2025-05-11T09:24:02.701753Z","shell.execute_reply.started":"2025-05-11T09:24:02.684140Z","shell.execute_reply":"2025-05-11T09:24:02.700854Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## RAG pipeline","metadata":{"id":"A9eoywr7_Oln"}},{"cell_type":"markdown","source":"TODO 2: Implement the RAG pipeline.","metadata":{"id":"8HDOjNYJ_Oln"}},{"cell_type":"markdown","source":"Please refer to the homework description slides for hints.\n\nAlso, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!","metadata":{"id":"MRGNa-1i_Oln"}},{"cell_type":"markdown","source":"- Naive approach (simple baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)","metadata":{"id":"cMaIsKAZ_Olo"}},{"cell_type":"markdown","source":"- Naive RAG approach (medium baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)","metadata":{"id":"mppO-oOO_Olo"}},{"cell_type":"markdown","source":"- RAG with agents (strong baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)","metadata":{"id":"HYxbciLO_Olo"}},{"cell_type":"code","source":"async def pipeline(question: str) -> str:\n    # TODO: Implement your pipeline.\n    # Currently, it only feeds the question directly to the LLM.\n    # You may want to get the final results through multiple inferences.\n    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n    #return qa_agent.inference(question)\n    # TODO: Implement your pipeline.\n    # Currently, it only feeds the question directly to the LLM.\n    # You may want to get the final results through multiple inferences.\n    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n    print(\"原始問題: \", question)\n    extracted_question = question_extraction_agent.inference(question)\n    print(\"簡化問題: \", extracted_question)\n    \n    # 使用plan_agent決定處理方式\n    plan = plan_agent.inference(extracted_question)\n    print(\"問題類型: \", plan)\n    \n    # 根據問題類型執行不同策略\n    if plan == \"direct_response\":\n        print(\"使用直接回應\")\n        response = direct_response_agent.inference(extracted_question)\n        # 如果direct_response_agent表示需要查證，改用web_search方式\n        if response == \"這個問題需要查證\":\n            print(\"需要查證，改用網路搜索\")\n            plan = \"web_search\"\n        else:\n            return response\n    \n    if plan == \"math\":\n        print(\"處理數學問題\")\n        return math_agent.inference(extracted_question)\n    \n    elif plan == \"web_search\":\n        print(\"使用網路搜索\")\n        # 提取關鍵字\n        keyword = keyword_extraction_agent.inference(extracted_question)\n        print(\"keyword: \", keyword)\n        \n        # 搜尋並處理結果\n        docs = []\n        search_result = await search(keyword)\n        for result in search_result:\n            try:\n                #import ipdb; ipdb.set_trace()\n                #print(\"judgement_result: \", result)\n                result_judgment = result_judgment_agent.inference(\"問題：\" + extracted_question + \"\\n文件內容：\" + result)\n                print(\"result_judgment: \", result_judgment[:200] + \"...\" if len(result_judgment) > 50 else result_judgment)\n                if result_judgment != \"False\":\n                    docs.append(result_judgment)\n            except ValueError as e:\n                if \"exceed context window\" in str(e):\n                    print(f\"文件太長，超出上下文窗口限制: {e}\")\n                    continue\n                else:\n                    print(f\"其它错误: {e}\")\n                    continue\n                    #raise e\n        \n        if not docs:\n            return \"沒有找到相關資料\"\n        \n        # 回答問題\n        return qa_agent.inference(\"docs: \" + str(docs) + \"\\nquestion: \" + extracted_question)\n        ","metadata":{"id":"ztJkA7R7_Olo","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:24:02.702725Z","iopub.execute_input":"2025-05-11T09:24:02.703030Z","iopub.status.idle":"2025-05-11T09:24:02.721201Z","shell.execute_reply.started":"2025-05-11T09:24:02.702999Z","shell.execute_reply":"2025-05-11T09:24:02.720348Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Answer the questions using your pipeline!","metadata":{"id":"P_kI_9EGB0S9"}},{"cell_type":"markdown","source":"Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process.","metadata":{"id":"PN17sSZ8DUg7"}},{"cell_type":"code","source":"from pathlib import Path\n\n# Fill in your student ID first.\nSTUDENT_ID = \"20250508\"\n\nSTUDENT_ID = STUDENT_ID.lower()\nwith open('./public.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    questions = [l.strip().split(',')[0] for l in questions]\n    for id, question in enumerate(questions, 1):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)\n\nwith open('./private.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    for id, question in enumerate(questions, 31):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n            print(answer, file=output_f)","metadata":{"id":"plUDRTi_B39S","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T09:24:02.722142Z","iopub.execute_input":"2025-05-11T09:24:02.722474Z","execution_failed":"2025-05-11T09:33:27.865Z"}},"outputs":[{"name":"stdout","text":"原始問題:  校歌為學校（包括小學、中學、大學等）宣告或者規定的代表該校的歌曲。用於體現該校的治學理念、辦學理想等學校文化。「虎山雄風飛揚」是哪間學校的校歌歌詞？\n簡化問題:  虎山雄風飛揚是哪間學校的校歌？\n問題類型:  web_search\n使用網路搜索\nkeyword:  虎山雄風飛揚 校歌\n文件太長，超出上下文窗口限制: Requested tokens (368024) exceed context window of 16384\nresult_judgment:  False\nresult_judgment:  根據文件內容，中興新村第一國民學校（今光華 國小）的校歌是「貓羅溪旁虎山雄風飛揚」。\n1 中興新村第一國民學校（今光華 國小）的校歌是「貓羅溪旁虎山雄風飛揚」。\n原始問題:  2025年初，NCC透過行政命令，規定民眾如果透過境外郵購無線鍵盤、滑鼠、藍芽耳機..等自用產品回台，每案一律加收審查費多少錢？\n簡化問題:  2025年初，NCC規定民眾透過境外郵購無線鍵盤、滑鼠和藍芽耳機等自用產品回台，每案加收審查費多少錢？\n問題類型:  web_search\n使用網路搜索\nkeyword:  NCC 農委會 境外郵購 審查費\nresult_judgment:  False\nresult_judgment:  根據文件內容，NCC規定民眾透過境外郵購無線鍑盤、滑鼠和藍芽耳機等自用產品回台，每案加收審查費750元。...\nresult_judgment:  文件內容提到，NCC今年2月起新增審查費，每案新台幣750元，這項政策引發質疑。國民黨立法院党团書記长王鴻薇表示，这是变相关税务，对人民造成负担；国 民 党 立 法 院 议员 廖 先 翔 已 发 文 要求 NCC 暂停收费新制。\n\n文件內容提到，N CC 历史上预告电信管理业务规则费用标准第14条修正草案，这项政策将对民众购买2部以下自用第二级通信管控射频器材（如手機、蓝牙耳机等）收取750元审查...\n2 根據文件內容，NCC規定民眾透過境外郵購無線鍑盤、滑鼠和藍芽耳機等自用產品回台，每案加收審查費750元。\n原始問題:  第一代 iPhone 是由哪位蘋果 CEO 發表？\n簡化問題:  第一代 iPhone 是由史蒂夫·乔布斯（Steve Jobs）發表。\n問題類型:  direct_response\n使用直接回應\n3 是的，第一代 iPhone 是由史蒂夫·乔布斯發表。\n原始問題:  台灣大學進階英文免修申請規定中，托福網路測驗 TOEFL iBT 要達到多少分才能申請？\n簡化問題:  托福網路測驗 TOEFL iBT 要達到多少分才能申請台灣大學進階英文免修？\n問題類型:  web_search\n使用網路搜索\nkeyword:  托福網路測驗 TOEFL iBT 分數 免修條件 台大進階英文\nresult_judgment:  文件內容雖然提到了TOEFL iBT，但主要是針對台灣大學的免修政策進行說明，並非直接回答問題所需關鍵資訊。因此，評估結果為：\n\nFalse...\nresult_judgment:  根據文件內容，我們可以看到關於托福網路測驗 TOEFL iBT 的相關資訊。然而，這個問題是問什麼分數才能申請台灣大學進階英文免修，而不是直接詢问TOFEL的門檻。\n\n在文中提到，各大一新生可以藉由優異的大一下學期成績來向學校申请大学英语必选课程（即进阶）或抵减学分。具体来说，大部分学校都有规定，如果考取多益750以上、托福iBT78上，或雅思6.0，則可申請免修英文課。\n\n因此，我們可以得出结论...\nresult_judgment:  根據文件內容，托福網路測驗 TOEFL iBT 要達到 92 分才能申請台灣大學進階英文免修。\n4 托福網路測驗 TOEFL iBT 要達到 78 分才能申請台灣大學進階英文免修。\n原始問題:  Rugby Union 中觸地 try 可得幾分？\n簡化問題:  Rugby Union 中觸地 try 可得幾分？\n問題類型:  math\n處理數學問題\n5 解題思路和計算步驟：  1. Rugby Union 的得分規則中，觸地 try 是一種特殊的試煉方式。 2。根據RugbyUnion官方網站提供的一般資訊，一個成功完成触底试验（touchdown）後，可以獲得五个点。  因此，我們可以直接結論：在 Rugby Union 中，每一次 触 地try 都可得 5 分  最終答案是： 觸地 try 可以得到 **_** _._.\n原始問題:  卑南族是位在臺東平原的一個原住民族，以驍勇善戰、擅長巫術聞名，曾經統治整個臺東平原。相傳卑南族的祖先發源自 ruvuwa'an，該地位於現今的哪個行政區劃？\n簡化問題:  ruvuwa'an 位於現今的哪個行政區劃？\n問題類型:  web_search\n使用網路搜索\nkeyword:  ruvuwa'an 行政區劃\nresult_judgment:  False\nresult_judgment:  根据文件内容，ruvuwa'an 位於現今的哪個行政區劃？无法在文中找到与 \"Ruvuvaan\" 相关信息。...\nresult_judgment:  False\n6 無法找到與 \"Ruvuwa'an\" 相關的信息。\n原始問題:  熊信寬，藝名熊仔，是臺灣饒舌創作歌手。2022年獲得第33屆金曲獎最佳作詞人獎，2023年獲得第34屆金曲獎最佳華語專輯獎。請問熊仔的碩班指導教授為？\n簡化問題:  熊仔的碩班指導教授為？\n問題類型:  web_search\n使用網路搜索\nkeyword:  熊仔 碩班 指導教授\nresult_judgment:  根據文件內容，該文檔直接相關於問題的核心要素（熊仔和指導教授），因此判斷為有幫助。\n\n過濾掉不必要的情況描述，只保留關鍵資訊：\n\n李琳山...\nresult_judgment:  False\nresult_judgment:  文件內容包含了許多與熊仔相關的資訊，但並不直接回答問題所需關鍵信息：\"什麼是他的碩士指導教授?\"\n\n因此，判斷結果為：\n\nFalse...\n7 李琳山\n原始問題:  請問誰發現了電磁感應定律，並奠定了電磁學的重要基礎？\n簡化問題:  詢問目標：發現電磁感應定律的科學家\n問題類型:  web_search\n使用網路搜索\nkeyword:  發現電磁感應定律的科學家\nresult_judgment:  法拉第电磁感应定律是描述電動勢與通過一閉合路徑的變化中的靜止或運動中之通量關係的一個基本物理學原理。\n文件太長，超出上下文窗口限制: Requested tokens (18275) exceed context window of 16384\nresult_judgment:  根據評估標準，文件內容與問題相關性很高。因此，我們可以過濾掉不必要的資訊，只保留有關發現電磁感應定律科學家的重要信息。\n\n法拉第是最早觀察到「电动机」现象的人，他在1821年9月11日发表了关于这一发现的心得。然而，这一事件也引起了一些争议，因为有人认为他抄袭沃尔斯顿的工作。这导致他的研究中断，但后来又重新开始。\n\n法拉第最终成功地观察到电磁感应现象，并在1831年8月29日发表了这一发现。他使用...\n8 迈克尔·法拉第\n原始問題:  臺灣東部第一座國家級博物館—「國立臺灣史前文化博物館」，源自於卑南文化遺址的發現。博物館擁有世界級建築設計與豐富的展品。請問距離國立臺灣史前文化博物館最近的臺鐵車站為？\n簡化問題:  國立臺灣史前文化博物館最近的臺鐵車站為？\n問題類型:  direct_response\n使用直接回應\n9 國立臺灣史前文化博物館最近的臺鐵車站是台中火车总站在。\n原始問題:  20+30=?\n簡化問題:  20+30=?\n問題類型:  math\n處理數學問題\n10 解題思路和計算步驟：  1. 問卷中給出的數字是20及30。 2. 我們需要將這兩個整数相加。  運算過程如下：    ① 將10進制的十位與一 位進行合併：(0+3)×（100）=300 ② 合並後，取出小數點前的部分，即可得答案  最終結果是：  20 +30 =50\n原始問題:  在NBA 2025年的交易大限前，達拉斯獨行俠隊的當家球星Luka Doncic被交易至聯盟中的哪一隊？\n簡化問題:  達拉斯獨行俠隊的Luka Doncic在NBA 2025年的交易大限前被哪一支球队收購？\n問題類型:  web_search\n使用網路搜索\nkeyword:  Luka Doncic NBA 2025交易大限\nresult_judgment:  文件內容：2025交易大限落幕！Doncic換AD震撼聯盟專家解析誰賺谁虧\n\n關鍵資訊：\n\n達拉斯獨行俠隊在NBA 2024-25年賽季的中期進行了Luka Dončić與Anthony Davis之間的大交易。雖然 AnthonyDavis依舊是個優秀球員，但失去 LUKA DONCIC 對於 Dallas Mavericks 的傷害難以彌補。\n\n因此，達拉斯獨行俠隊在2025年NBA 交流大限...\nresult_judgment:  False\nresult_judgment:  達拉斯獨行俠隊的Luka Doncic在NBA 2025年的交易大限前被哪一支球队收購？\n\n文件內容提到，洛杉磯湖人與猶他爵士、夏威夷太陽三方進行了一項重大交換。達拉斯獨行俠隊的Luka Doncic在2025年2月3日被交易至了 洛薩格比克蘭人的手中...\n11 洛薩格比克蘭人\n原始問題:  請問2024年美國總統大選的勝選人是誰？\n簡化問題:  2024年美國總統大選的勝出者是誰？\n問題類型:  web_search\n使用網路搜索\nkeyword:  2024年 美國 總統 大選 勝出者\n文件太長，超出上下文窗口限制: Requested tokens (26566) exceed context window of 16384\nresult_judgment:  文件內容與問題相關性評估：\n\n1. 文件直接關於2024年美國總統大選的勝出者。\n2.file中提到特朗普在美国总统选举中的压倒性的胜利，獲得了312票对226张投给哈里斯。\n\n因此，我們可以保留以下內容作為答案：\n\n\"布雷德迈尔共和党候补人、前總統唐纳得· 特隆 普以及梅拉尼娅 ·特朗普和巴伦 2024年11月6日抵达佛罗来州西棕榆滩参加选举之夜派对并发表讲话。分享在美国总统大選中获得压倒性胜利...\nresult_judgment:  根據評估標準，文件內容與問題相關性如下：\n\n1. 文件直接關於2024年美國總統大選的勝出者是誰。\n2.file中提到兩個主要候选人：唐納德·特朗普（共和黨）和賀錦麗 (民主党)。\n\n因此，文件內容包含了回答問題所需的一些相關資訊。以下為精簡後對答案：\n\n2024年美國總統大選的勝出者是誰？根據目前的情況，這兩個主要候选人分別为唐納德·特朗普（共和黨）和賀錦麗 (民主党)...\n12 唐納德·特朗普\n原始問題:  Meta 的 Llama-3.2 系列模型中，參數量最小的模型是多少 Billion 的參數？\n簡化問題:  Meta 的 Llama-3.2 系列模型中，參數量最小的モデル是多少 Billion 參数？\n問題類型:  math\n處理數學問題\n13 解題思路：  1. 了解 Llama-3 系列模型的參數量：Llamas 是一種基於 Transformers 的語言模式，具有一系列不同大小和複雜性的版本。 2 查找相關資訊以確定哪個是最小型別。  計算步驟： 查詢 Meta 公開資料庫或研究論文，以找到 Llama-3.0 系列中參數量的信息。根據這些來源，我們發現Llamas 1、2和mini等版本都有不同的参数数量。 最终答案是：  根据Meta提供的一份报告，LLama-mini模型約為137億個参数。  因此，最小型別 Llama-3.0 系列的參數量大约为13亿个。\n原始問題:  停修是指在學期中，學生因為各式因素認為無法繼續修習某一門課時，可以向學校申請停修。與學期初選課退選不同，停修有更嚴格的限制。依據國立臺灣大學學則，在沒有學生報告書的情況下，學生每學期至多停修幾門課程？\n簡化問題:  停修是指在學期中，依據國立臺灣大學的規定，有關限制多少門課程？\n問題類型:  web_search\n使用網路搜索\nkeyword:  停修 是指 在學期中 依據 國立臺灣大學 規定 有關限制 多少 門課程\nresult_judgment:  停修是指在學期中，依據國立臺灣大學的規定，有關限制多少門課程？\n\n根 據文件內容： \n\n一、相關法令及網址：\n 國 立 臺 灣 大 學 生 停 修 課 程 辦 法\n\n二、一般情況下，每個生都可以停修一個學期中的任意數目的科目，但有以下限制：\n\n（三）停止課程仍須登記於該学年成績單及歷年的成绩表，于 成绩栏注明「 停 修 」。 \n\n因此，可以得出结論：在國立臺灣大學，每個生可以停修一個學期中的任意數...\nresult_judgment:  False\nresult_judgment:  停修是指在學期中，依據國立臺灣大學的規定，有關限制多少門課程？根 據文件內容，我們可以得知：\n\n* 停 修 每 學 期 以 不 超 過 3 個 課 程 為 限。\n\n因此，這個問題答案為：不超過三學分。...\n14 根據文件內容，我們可以得知：停修每學期以不超過3個課程為限。\n原始問題:  DeepSeek公司的母公司是？\n簡化問題:  DeepSeek公司的母company是？\n問題類型:  web_search\n使用網路搜索\nkeyword:  DeepSeek 公司 母公司\nresult_judgment:  根據評估標準，文件內容與問題相關性不大。\nresult_judgment:  DeepSeek的母公司是幻方（Fantang），是一家中国对冲基金。\nresult_judgment:  False\n15 幻方（Fantang）\n原始問題:  2024年NBA的總冠軍隊伍是哪一隊？\n簡化問題:  2024 年 NBA 總冠軍隊伍是哪一队？\n問題類型:  web_search\n使用網路搜索\nkeyword:  2024 NBA 總冠軍隊伍\nresult_judgment:  True\n\n2024年NBA總冠軍隊伍是波士頓塞爾提克。\nresult_judgment:  2024年NBA总决赛队伍教练胜场数波士顿凯尔特人乔·马兹拉达： 64-18（.780）大西洋组第1名，东区第一位，全联盟首席。例行賽50勝32敗( .610) 西南組第二 名、全聯盟第三 位。\n\n2024年NBA总决赛队伍教练胜场数达拉斯独 行侠杰森·基德： 44-38（.537）西区第五名，全联盟第四位。...\nresult_judgment:  文件內容：凯尔特人夺得队史第18个NBA总冠军\n\n本文直接相關於問題的核心要素（2024 年 NBA 總 冠軍隊伍），因此，過濾掉不必要的情況，只保留關鍵資訊：\n\n波士顿凱爾 特 人 队 在 总决赛第五场主場 106:88 大勝达拉斯独行侦队，以总比分4：1 赢得隊史 第18 个NBA總 冠軍。...\n16 波士頓塞爾提克。\n原始問題:  一個碳氫化合物分子中有碳原子與碳原子形成三鍵，此類化合物稱為什麼？\n簡化問題:  三鍵碳氫化合物\n問題類型:  web_search\n使用網路搜索\nkeyword:  三鍵碳氫化合物\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Combine the results into one file.\nwith open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n    for id in range(1,91):\n        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n            answer = input_f.readline().strip()\n            print(answer, file=output_f)","metadata":{"id":"GmLO9PlmEBPn","trusted":true,"execution":{"execution_failed":"2025-05-11T09:33:27.865Z"}},"outputs":[],"execution_count":null}]}